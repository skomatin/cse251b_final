{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2fee74f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import torchtext as text\n",
    "import sys\n",
    "import tqdm\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import string\n",
    "from torchtext.data import get_tokenizer\n",
    "from vocab import *\n",
    "from utils import *\n",
    "from constants import *\n",
    "import pickle\n",
    "\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e34185c3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset squad (/tmp/xdg-cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62eb6c1bc7d04949a1dc9c0f61770e80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset(\"squad\")\n",
    "\n",
    "train_dic = {'passage': [], 'question': [], 'answer': []}\n",
    "for i in range(len(dataset['train'])):\n",
    "    datum = dataset['train'][i]\n",
    "    for j in range(len(datum['answers']['text'])):\n",
    "        train_dic['passage'].append(datum['context'])\n",
    "        train_dic['question'].append(datum['question'])\n",
    "        train_dic['answer'].append(datum['answers']['text'][j])\n",
    "\n",
    "train = pd.DataFrame(train_dic)\n",
    "\n",
    "val_dic = {'passage': [], 'question': [], 'answer': []}\n",
    "for datum in dataset['validation']:\n",
    "    for elem in datum['answers']['text']:\n",
    "        ans_id = 0\n",
    "        val_dic['passage'].append(datum['context'])\n",
    "        val_dic['question'].append(datum['question'])\n",
    "        val_dic['answer'].append(elem)\n",
    "\n",
    "val = pd.DataFrame(val_dic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7723ace",
   "metadata": {},
   "source": [
    "## Regular Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70fa0515",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tqdm(range(len(train))):\n",
    "    row = train.iloc[i]\n",
    "    passage = clean_text(row['passage'].lower())\n",
    "    question = clean_text(row['question'].lower())\n",
    "    answer = clean_text(row['answer'].lower())\n",
    "    \n",
    "    train.iloc[i]['passage'] = passage\n",
    "    train.iloc[i]['question'] = question\n",
    "    train.iloc[i]['answer'] = answer\n",
    "    \n",
    "for i in tqdm(range(len(val))):\n",
    "    row = val.iloc[i]\n",
    "    passage = clean_text(row['passage'].lower())\n",
    "    question = clean_text(row['question'].lower())\n",
    "    answer = clean_text(row['answer'].lower())\n",
    "    \n",
    "    val.iloc[i]['passage'] = passage\n",
    "    val.iloc[i]['question'] = question\n",
    "    val.iloc[i]['answer'] = answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5059d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split Val into val and test\n",
    "\n",
    "val = val.sample(frac=1).reset_index(drop=True)\n",
    "test = val[:10000]\n",
    "val = val[10000:]\n",
    "\n",
    "train.to_csv('./data/train.csv', index=False)\n",
    "val.to_csv('./data/val.csv', index=False)\n",
    "test.to_csv('./data/test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61acf74",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = build_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa2709d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = load_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c550047a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://nlp.stanford.edu/data/glove.6B.zip\n",
    "!unzip glove.6B.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33646e70",
   "metadata": {},
   "source": [
    "## Building Embedding Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f275851e",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_mat = np.random.randn(len(vocab), 50)\n",
    "with open('glove.6B.50d.txt', 'r') as f:\n",
    "    for line in f.readlines():\n",
    "        info = line.split(' ')\n",
    "        word = info[0]\n",
    "        \n",
    "        if word not in vocab.word2idx.keys():\n",
    "            continue\n",
    "        idx = vocab.word2idx[word]\n",
    "        embed_mat[idx] = np.array(info[1:], dtype='float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d800fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1efeba7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('embeddings.npy', embed_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa4bde5f",
   "metadata": {},
   "source": [
    "#### Method for using masked passages to indicate answer presence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27645cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_processed_data_mask(df, tokenizer):\n",
    "        data = []\n",
    "        for idx in tqdm(range(len(df))):\n",
    "            pass_tokens = ['<start>'] + tokenizer(df.iloc[idx][\"passage\"]) + ['<end>']\n",
    "            ans_tokens = ['<start>'] + tokenizer(df.iloc[idx][\"answer\"]) + ['<end>']\n",
    "            q_tokens = ['<start>'] + tokenizer(df.iloc[idx][\"question\"]) + ['<end>']\n",
    "            \n",
    "            pass_len = MAX_PASSAGE_LEN + 2 # +2 for start and end tokens\n",
    "            ans_len = MAX_ANSWER_LEN + 2\n",
    "            q_len = MAX_QUESTION_LEN + 2\n",
    "            \n",
    "            pass_mask = torch.zeros(pass_len)\n",
    "            temp_pass = tokenizer(df.iloc[idx]['passage'])\n",
    "            temp_ans = tokenizer(df.iloc[idx]['answer'])\n",
    "            for i in range(len(temp_pass)):\n",
    "                elem = temp_pass[i]\n",
    "                if elem in temp_ans:\n",
    "                    pass_mask[i+1] = 1 # shift by 1 for start token\n",
    "            pass_mask = pass_mask.float()\n",
    "\n",
    "            passage = [vocab(word) for word in pass_tokens]\n",
    "            answer = [vocab(word) for word in ans_tokens]\n",
    "            question = [vocab(word) for word in q_tokens]\n",
    "\n",
    "            # padding to same length\n",
    "            pass_idxs = torch.zeros(pass_len)\n",
    "            ans_idxs = torch.zeros(ans_len)\n",
    "            q_idxs = torch.zeros(q_len)\n",
    "\n",
    "            pass_idxs[:len(passage)] = torch.FloatTensor(passage)\n",
    "            ans_idxs[:len(answer)] = torch.FloatTensor(answer)\n",
    "            q_idxs[:len(question)] = torch.FloatTensor(question)\n",
    "\n",
    "            data.append((pass_idxs, ans_idxs, q_idxs, pass_mask))\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2321b55a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "\n",
    "# train_processed = get_processed_data_mask(train, tokenizer)\n",
    "# with open('./data/train_processed_mask.pickle', 'wb') as train_file:\n",
    "#     pickle.dump(train_processed, train_file)\n",
    "\n",
    "val = pd.read_csv('./data/val.csv')\n",
    "val_processed = get_processed_data_mask(val, tokenizer)\n",
    "with open('./data/val_processed_mask.pickle', 'wb') as val_file:\n",
    "    pickle.dump(val_processed, val_file)\n",
    "\n",
    "\n",
    "test = pd.read_csv('./data/test.csv')\n",
    "test_processed = get_processed_data_mask(test, tokenizer)\n",
    "with open('./data/test_processed_mask.pickle', 'wb') as test_file:\n",
    "    pickle.dump(test_processed, test_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fe4a599",
   "metadata": {},
   "source": [
    "## Save Pickle File for Regular Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27ec51d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_processed_data(df, tokenizer):\n",
    "        data = []\n",
    "        for idx in tqdm(range(len(df))):\n",
    "                pass_tokens = ['<start>'] + tokenizer(df.iloc[idx][\"passage\"]) + ['<end>']\n",
    "                ans_tokens = ['<start>'] + tokenizer(df.iloc[idx][\"answer\"]) + ['<end>']\n",
    "                q_tokens = ['<start>'] + tokenizer(df.iloc[idx][\"question\"]) + ['<end>']\n",
    "                # pass_tokens = ['<start>'] + list(map(tokenizer, df.iloc[idx][\"passage\"])) + ['<end>']\n",
    "                # ans_tokens = ['<start>'] + list(map(tokenizer, df.iloc[idx][\"answer\"])) + ['<end>']\n",
    "                # q_tokens = ['<start>'] + list(map(tokenizer, df.iloc[idx][\"question\"])) + ['<end>']\n",
    "\n",
    "                pass_len = MAX_PASSAGE_LEN + 2 # +2 for start and end tokens\n",
    "                ans_len = MAX_ANSWER_LEN + 2\n",
    "                q_len = MAX_QUESTION_LEN + 2\n",
    "\n",
    "                passage = [vocab(word) for word in pass_tokens]\n",
    "                answer = [vocab(word) for word in ans_tokens]\n",
    "                question = [vocab(word) for word in q_tokens]\n",
    "\n",
    "                # padding to same length\n",
    "                pass_idxs = torch.zeros(pass_len)\n",
    "                ans_idxs = torch.zeros(ans_len)\n",
    "                q_idxs = torch.zeros(q_len)\n",
    "\n",
    "                pass_idxs[:len(passage)] = torch.FloatTensor(passage)\n",
    "                ans_idxs[:len(answer)] = torch.FloatTensor(answer)\n",
    "                q_idxs[:len(question)] = torch.FloatTensor(question)\n",
    "\n",
    "                data.append((pass_idxs, ans_idxs, q_idxs))\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d03b997",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "train_processed = get_processed_data(train, tokenizer)\n",
    "val_processed = get_processed_data(val, tokenizer)\n",
    "test_processed = get_processed_data(test, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d771f36c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/train_processed.pickle', 'wb') as train_file:\n",
    "    pickle.dump(train_processed, train_file)\n",
    "\n",
    "with open('./data/val_processed.pickle', 'wb') as val_file:\n",
    "    pickle.dump(val_processed, val_file)\n",
    "\n",
    "with open('./data/test_processed.pickle', 'wb') as test_file:\n",
    "    pickle.dump(test_processed, test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a29b9d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "147ef1215b7e2b4bf3a64983c233460acb54149cdc3836f93d9a84ff7ba2f913"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
