{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71fb263e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import torchtext as text\n",
    "import sys\n",
    "import tqdm\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import string\n",
    "from torchtext.data import get_tokenizer\n",
    "from vocab import *\n",
    "from utils import *\n",
    "from constants import *\n",
    "import pickle\n",
    "\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3de0ed9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset squad (/tmp/xdg-cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68b4cf5788f24e25b9e75e756ca4d846",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset(\"squad\")\n",
    "\n",
    "train = {'passage': [], 'question': [], 'answer': []}\n",
    "for i in range(len(dataset['train'])):\n",
    "    datum = dataset['train'][i]\n",
    "    for j in range(len(datum['answers']['text'])):\n",
    "        train['passage'].append(datum['context'])\n",
    "        train['question'].append(datum['question'])\n",
    "        train['answer'].append(datum['answers']['text'][j])\n",
    "\n",
    "train = pd.DataFrame(train)\n",
    "\n",
    "val = {'passage': [], 'question': [], 'answer': []}\n",
    "for datum in dataset['validation']:\n",
    "    for elem in datum['answers']['text']:\n",
    "        ans_id = 0\n",
    "        val['passage'].append(datum['context'])\n",
    "        val['question'].append(datum['question'])\n",
    "        val['answer'].append(elem)\n",
    "\n",
    "val = pd.DataFrame(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "39705f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_short_dic = {'passage': [], 'answer': [], 'question': []}\n",
    "for idx in range(len(train)):\n",
    "    row = train.iloc[idx]\n",
    "    passage, answer, question =  row['passage'].lower(), row['answer'].lower(), row['question'].lower()\n",
    "    \n",
    "    prefix, ans, suffix = passage.partition(answer)\n",
    "    context = ' '.join(prefix.split(' ')[-10:]) + ans + ' '.join(suffix.split(' ')[:10])\n",
    "    train_short_dic['passage'].append(context)\n",
    "    train_short_dic['answer'].append(answer)\n",
    "    train_short_dic['question'].append(question)\n",
    "train_short = pd.DataFrame(train_short_dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90ca2491",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_short_dic = {'passage': [], 'answer': [], 'question': []}\n",
    "for idx in range(len(val)):\n",
    "    row = val.iloc[idx]\n",
    "    passage, answer, question =  row['passage'].lower(), row['answer'].lower(), row['question'].lower()\n",
    "    \n",
    "    prefix, ans, suffix = passage.partition(answer)\n",
    "    context = ' '.join(prefix.split(' ')[-10:]) + ans + ' '.join(suffix.split(' ')[:10])\n",
    "    val_short_dic['passage'].append(context)\n",
    "    val_short_dic['answer'].append(answer)\n",
    "    val_short_dic['question'].append(question)\n",
    "val_short = pd.DataFrame(val_short_dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5653498b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split Val into val and test\n",
    "\n",
    "val_short = val_short.sample(frac=1).reset_index(drop=True)\n",
    "test_short = val_short[:10000]\n",
    "val_short = val_short[10000:]\n",
    "\n",
    "train_short.to_csv('./data/train_short.csv', index=False)\n",
    "val_short.to_csv('./data/val_short.csv', index=False)\n",
    "test_short.to_csv('./data/test_short.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af2b0465",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded vocab\n"
     ]
    }
   ],
   "source": [
    "vocab = load_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "76d8347a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "68"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = get_tokenizer('basic_english')\n",
    "train_short['passage'].apply(lambda x: len(tokenizer(x))).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "03cce2ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_processed_short_data(df, tokenizer):\n",
    "        data = []\n",
    "        for idx in tqdm(range(len(df))):\n",
    "            pass_tokens = ['<start>'] + tokenizer(df.iloc[idx][\"passage\"]) + ['<end>']\n",
    "            ans_tokens = ['<start>'] + tokenizer(df.iloc[idx][\"answer\"]) + ['<end>']\n",
    "            q_tokens = ['<start>'] + tokenizer(df.iloc[idx][\"question\"]) + ['<end>']\n",
    "\n",
    "            pass_len = MAX_SHORT_PASSAGE_LEN + 2 # +2 for start and end tokens\n",
    "            ans_len = MAX_SHORT_ANSWER_LEN + 2\n",
    "            q_len = MAX_SHORT_QUESTION_LEN + 2\n",
    "\n",
    "            passage = [vocab(word) for word in pass_tokens]\n",
    "            answer = [vocab(word) for word in ans_tokens]\n",
    "            question = [vocab(word) for word in q_tokens]\n",
    "\n",
    "            # padding to same length\n",
    "            pass_idxs = torch.zeros(pass_len)\n",
    "            ans_idxs = torch.zeros(ans_len)\n",
    "            q_idxs = torch.zeros(q_len)\n",
    "\n",
    "            pass_idxs[:len(passage)] = torch.FloatTensor(passage)\n",
    "            ans_idxs[:len(answer)] = torch.FloatTensor(answer)\n",
    "            q_idxs[:len(question)] = torch.FloatTensor(question)\n",
    "\n",
    "            data.append((pass_idxs, ans_idxs, q_idxs))\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fb49a594",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2cfa21d02e974c129f19604feef6c22b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/87599 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d22ab9c80593477bb9dcf7ff7c642b8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/24726 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6278e9a72124a7aac3662132d8ceeb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "train_short_processed = get_processed_short_data(train_short, tokenizer)\n",
    "val_short_processed = get_processed_short_data(val_short, tokenizer)\n",
    "test_short_processed = get_processed_short_data(test_short, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "80fdfb14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([70])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_short_processed[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "124d24c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/train_short_processed.pickle', 'wb') as train_file:\n",
    "    pickle.dump(train_short_processed, train_file)\n",
    "\n",
    "with open('./data/val_short_processed.pickle', 'wb') as val_file:\n",
    "    pickle.dump(val_short_processed, val_file)\n",
    "\n",
    "with open('./data/test_short_processed.pickle', 'wb') as test_file:\n",
    "    pickle.dump(test_short_processed, test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c50526c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/train_short_processed.pickle', 'rb') as train_file:\n",
    "    temp = pickle.load(train_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c2909557",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([70])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp[0][0].shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (clean)",
   "language": "python",
   "name": "python3_clean"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
